\section{Porting Challenges}

Although the pre-exascale machines provided good experience to the \vtkm development team for porting to different processor architectures, the design of the Frontier and Aurora exascale machines introduced new technical challenges.
This section reports the most significant modifications required to make it feasible to run \vtkm on exascale hardware.

%% \ken{
%%   Each subsection should be roughly 1/3 page.
%%   (1 + 1/3 page total.)
%%   The subsection should start with a paragraph defining the problem/motivation.
%%   The following paragraph should give an overview of the approach.
%%   The remaining paragraphs can go into technical challenges that were encountered and how they were addressed.
%% }

\subsection{Adopting Kokkos}
\label{sec:adopting-kokkos}

%\assign{Sujin}

%\sujin{
%The first two paragraphs can be simplified by referring to the device adapter explanation in the \vtkm overview section
%}
As described in Section \ref{sec:overview},
one of the main features that \vtkm supports is ``write once, compile anywhere'' for its algorithms.
%\vtkm filter's are implemented in modern C++. The build system of \vtkm can then compile these for one or more of the various parallel devices that it supports. This level of compatibility is achieved through the use of an abstraction layer in \vtkm called the \texttt{DeviceAdapter}.
A device portability layer called a device adapter allows algorithms written with standard C++ features to run across all devices.
These device adapters wrap a parallel device into a common API that provides functionalities for parallel-task scheduling, memory management, atomic operations, and several commonly used parallel algorithms such as scan, sort, and reduce through a common interface.
These are implemented on top of the native libraries used to program these devices.
%At the most basic level, a device adapter only needs to provide support for memory management on the device, a way to execute an n-way, fine-grained, parallel task on the device, and atomic operations.
%There are generic implementations for all of the parallel algorithms like sort, reduce, scan, etc, but it is possible to override these with specialized implementations for best performance.
%The early versions of \vtkm had \texttt{DeviceAdapter} implementations for single threaded CPUs using standard C++, multithreaded CPUs using the threading building blocks library or OpenMP, and GPGPUs using CUDA.

When the architectures for Aurora and Frontier were announced, it was revealed that they would be using two completely new GPU architectures from two different vendors (i.e., Intel GPUs in Aurora and AMD GPUs in Frontier) with their own native libraries (SYCL for Intel and HIP for AMD).
Porting VTK-m directly to these new archictectures would have required two new device adapter back ends.
Although the device adapter layer greatly simplifies the porting of \vtkm code, implementing device adapters themselves is not trivial.
So, although it would be technically feasible to design device adapters for two new devices, it would have required significant effort.
%Maintaining these device adapters together with the already existing device adapters was would require a lot of effort, especially for a project that is primarily concerned with implementing highly parallel visualization algorithms.

To overcome this hurdle, we looked to another ECP project called Kokkos~\citep{Edwards2014, Trott2022}. Kokkos is a library for implementing performance-portable applications in C++. Similar to \vtkm, Kokkos also supports multiple device back ends, including SYCL and HIP, which are used by the exascale systems. So, with just one device adapter built on top of Kokkos, we were able to target both the machines.
This allows us to write one \vtkm device adapter to target the Kokkos API and defer the work of interfacing with the different ECP device APIs to the Kokkos team, who were already doing this for other ECP projects.

%\ken{Is there anything more to be said about the interface? Was there anything particularly challenging? Any mismatch between the APIs? Any ``hacks'' that might have happened?}
%\sujin{I have added a few paragraphs bellow. It may be too detailed and probably needs some simplification}
Because the exascale machines were based on completely new hardware, Kokkos was also in active development when we were developing the \vtkm device adapter. Therefore, we faced a few challenges during the development, and some of the most interesting challenges we encountered are described below.

As mentioned previously, \vtkm filters and algorithms are implemented as a sequence of some primitive parallel operations such as scan, sort, and reduce.
To achieve good performance, the implementation of these parallel primitives must be as efficient as possible.
\vtkm comes with generic implementations for these operations, but generality precludes sufficient optimization for target architectures.
Fortunately, device adapters specialize and optimize these operations for their target hardware.

For the Kokkos device adapter, we relied on the implementations provided by the Kokkos library.
However, we discovered that Kokkos did not have all of the primitives that \vtkm supported, and, even when available, some implementations were not as flexible as those supported natively by \vtkm.
For example, at the time of development, the Kokkos sort only supported floating point values with the less-than operator as the ordering comparator,
whereas \vtkm supported all primitive types and also non-primitive types with custom comparison operators.
Therefore, we had to implement code paths to use the Kokkos sort for only supported arguments and to fall back to the generic \vtkm implementations for all other cases.

Another issue we encountered were bugs in the Kokkos code base. During development, we hit several critical bugs that would crash the program. Therefore, we also had to work around these failing features with a fallback code path for failing conditions until these issues could be addressed.

We also faced challenges getting the Kokkos library initialization to work with \vtkm.
Kokkos only gives one option to specify configuration options at the library initalization during program startup.
In contrast, \vtkm's initialization is optional and allows the configuration to be selected more flexibly.
This required us to rethink and overhaul our own initialization procedure so that all of our dependencies were initialized correctly and in the correct order.
Furthermore, using \vtkm with other libraries that also use Kokkos is a valid use case, but only one of these libraries should initialize and finalize the Kokkos library.
To prevent it from ``stealing'' the Kokkos initialization from other code, \vtkm delays Kokkos initialization until it is required and detects whether the Kokkos system is already initialized.
If \vtkm does initialize Kokkos, then it establishes a callback at program termination to finalize the Kokkos library.

\subsection{Addition and Then Removal of Virtual Methods}
\label{sec:virtual-methods}

%\assign{Ken}

One of the challenges of a general purpose visualization library like \vtkm is that it is intended to support data coming from all manner of data producers.
It cannot make assumptions about data structures from the base data types (float vs double vs int) to the arrangement and interpolation of values in mesh structures.
As such, it requires sophisticated software engineering to enable flexibility and adaptability that is practical both for developers and users of \vtkm.

\vtkm uses C++ templates to customize algorithms for different data types.
Ideally, an algorithm
%that operates on arrays
will know the data types on which it will operate.
Unfortunately, this is seldom the case for \vtkm because data ingested from different sources can have any number of basic types
%(32- or 64-bit floats or fixed precision values encoded in integers)
and memory layouts.
In the early years of the ECP, this problem was addressed by compiling algorithms in \vtkm for all possible types that could be encountered.
However, the amount of potential cases generated is too many to be practical.

C++ objects with virtual methods are a natural solution to this problem, and when they became available on CUDA devices, \vtkm started using virtual methods to hide the structure of arrays.
This originally addressed some of the issues with type abstraction.
However, this solution worked poorly and was later abandoned for two reasons.
First, using virtual methods introduced problems with library linking because the compilers had to collect any possible code that might need to be loaded on the device.
In addition to the obvious problem of bloat from replicating object code across all libraries, the process required a fragile and slow build step, and it precluded the possibility of adding further subclasses after the library was built.
Second, when the ECP announced that its exascale machines would be using GPUs from Intel and AMD (i.e., not using NVIDIA's CUDA), it was unclear how well they would support virtual methods or even if they would support them at all.

Consequently, the \vtkm development team pivoted, and
virtual methods were removed from the \vtkm code that ran on devices.
To manage the operations on types without using virtual methods, \vtkm employed a trio of strategies: multiplexing the type in the algorithm, generalizing the stride in arrays, and providing fall backs when unexpected types were encountered.

The first strategy, multiplexing, required using a type-agnostic storage object.
For this, a \texttt{Variant} class was added to \vtkm.
The \texttt{Variant} takes a list of types as template parameters, and it can hold exactly one of these objects at a time.
At run time, the proper type can be queried and extracted.
%Implementing the \texttt{Variant} to avoid type punning across all device compilers was challenging.
Although multiplexing from a variant object still requires separate compilation for all possible types, it limits the code that must be recompiled to make it more manageable.

The second strategy required a redesign of the array management in \vtkm.
Where the original design of array management completely abstracted the implementation, the new design based the array management on raw buffers of memory that internally be reinterpreted as C arrays to implement different array types.
This in turn provided a way to generalize the representation of an array component by defining a stride in the buffer.
In this way, an algorithm no longer had to be compiled for a specific data layout.
It could instead apply a stride to the array and use any layout.

The third strategy recognized that although many types are possible, most are rarely encountered in practice.
Therefore, instead of attempting to compile a function for any possible type, it is possible to instead compile for the most likely types and then have a fallback for the cases when the type is unexpected.
The typical solution is to copy the array of an unknown type to an array of a known type.
This feature was included in the filter interface overhaul, which is described in the next section.

\subsection{Filter Interface Overhaul}
\label{sec:filter-overhaul}

%\assign{Ollie}

The majority of algorithms in \vtkm are contained in what is called a ``filter'' object.
A filter takes a dataset, performs some operations to modify it, and returns the resulting dataset.
Filters provide the outwardly facing API to process data in \vtkm.
However, as previously described, these datasets can have any number of data types and structures.
The \vtkm filter base class needs to provide a mechanism to resolve data types.
That is, the filter base class needs a way to call a templated method in a derived implementation class with fully resolved data types.

Because a C++ template method cannot also be a virtual function, which is needed for typical run-time polymorphism, the original design used a rather complicated technique called the curiously recurring template pattern (CRTP)~\citep{Coplien1995} to emulate it.
CRTP works by making the type of derived class a template argument to the base class.
When the base class needs to call a method in the derived class, rather than use a virtual method, it recasts itself as the derived class and calls the method directly.
This allows the base class to iterate through a list of supported data types and call a templated method in the derived class to implement the algorithm on each one.

%The addition of \texttt{Variant} and the redesign of array management in \vtkm also enabled us to further redesign the developer's interface for \texttt{Filter}s.
%Previously, a filter implementation had to provide a \texttt{DoExecute} method that took a template \texttt{ArrayHandle<T>} as one of its arguments.
%Filter base classes would iterate through a list of supported data types, defined by the filter developer, instantiate and call \texttt{DoExecute} with each of the concrete type \texttt{T}.
%There were several downsides of this design.
%First, as mentioned above, we had to instantiate a large number of \textit{DoExecute}, one for each of \texttt{T} in the list.
%Second, since a C++ template method can not also be a virtual function which is needed for runtime polymorphism, the original design used a rather complicated technique called \emph{Couriously recurring template pattern (CRTP)} to emulate it.

However, using CRTP also made the base filter class itself a class template, and its execution methods required exposed definitions that the compiler internally recompiled each time they were called elsewhere in the code.
This made the \vtkm filter library essentially a header-only library, and clients using it had to include all the necessary implementation in the header files.
When compiling client code, all those header files had to be parsed, and classes and methods had to be instantiated.
As a consequence, it took a long time to compile applications of \vtkm.
This issue was most prominent when compiling for GPU devices because most device compilers were not as efficient as host compilers when handling C++ templates.
When integrated as part of an in-situ pipeline, simulation code that called \vtkm's filters also had to be compiled by a device compiler, thus exacerbating the problem.
In some extreme cases, the entire compilation process took more than 24 hours to complete.

The new design changed the type-dispatching mechanism.
The responsibility of type dispatching was shifted from the filter base class to the derived implementation classes.
The execution method in the derived class is no longer a template; it accepts a dataset that contains ambiguous types.
This increases the burden on the algorithm developers who must now resolve types themselves.
To compensate, \vtkm provides an alternate dispatching mechanism by providing various forms of a ``cast-and-call'' mechanism on the dataset's elements.
The derived filter performs the type dispatching by passing a dataset element to a cast-and-call along with a templated, callable object.
This callable object is generally the type-dependent, core business logic of the filter implementation wrapped inside a C++14 generic lambda expression.
The lambda expression will be instantiated with types from a predefined type list by the cast-and-call mechanism.

The cast-and-call mechanism will attempt to resolve the data to a prescribed list of types.
When the data type is not part of the type list, the cast-and-call mechanism can fall back to a known type, as described in Section \ref{sec:virtual-methods}.
The predefined type list and fallback limit the number of instantiations of the lambda expression.
Using cast-and-call with lambda expressions in this way also limits the portion of code to be instantiated.
In contrast, the original design required the entire body of the filter implementation to be instantiated, which can increase both compilation time and code bloat.

Because the filter object methods are no longer template methods, they can be virtual methods.
Note that the filter methods in question are explicitly run in \vtkm's control environment, and this means they will only be run on the CPU host.
Previously discussed issues with virtual methods on GPUs do not apply here, so creating virtual methods is not problematic in this case.
Moreover, the virtual methods eliminate the need to use the CRTP technique.
Consequently, the entire filter class hierarchy has become a non-template class as well.
The library is no longer a header-only library and can be built as a traditional, precompiled library.
The filter library is further divided into several modules with each separately compiled into a \texttt{.so} file.
This allows subsequent linking to potentially load less memory from libraries.
It also allows users to turn off the compiling of libraries they do not need for faster compilation.
Finally, applications no longer need to be compiled by a device compiler simply because they are using \vtkm.

The new filter structure, which removes exposed templates and encapsulates code into libraries, greatly reduces application build time because applications no longer have to compile many \vtkm templates.
Rather, applications simply link to methods in the \vtkm library.
The new approach also provides other compile-time saving opportunities within the library.
For example, some filters incorporate the functionality of others as a subroutine.
The new filter structure allows filters to be compiled once and have their functionality leveraged by other filters.
For example, the compilation of \vtkm's material interface reconstruction filter was reduced by a factor of 9 by linking to the mesh quality filter rather than recompiling it.

The new filter structure also enables dividing the code into multiple translation units (i.e., separate C++ source files).
Although this does not necessarily reduce the aggregate compile times, it more effectively leverages cores in parallel builds and reduces the possibility of compiler crashes from running out of resources.
%\ollie{Ken can you add some concrete timing measure as you did before here?}
%\ken{I will look. I probably don't have anything concrete. I'm not sure I captured enough data to measure the time before the filter change and the time after. (A lot changed in the interim anyway.) But I think I can pull up a couple of examples.}

\subsection{GPU-to-GPU Transfers}

Modern GPUs used in HPC can perform direct GPU-to-GPU communication. This provides GPUs with an efficient mechanism to send data stored in their device memory directly to another GPU's device memory. This contrasts the traditional and costly GPU communication pattern that required first copying the desired data from the first GPU's device memory to the system's host memory and then copying the data again from the host memory to the second GPU's device memory.

Enabling this feature in \vtkm required changes in the source code of both \vtkm itself and DIY, which is a third-party library that \vtkm uses to manage its MPI (Message Passing Interface) communication~\citep{Peterka2011,Morozov2016}.
%The reason for making changes to DIY is that \vtkm delegates data marshaling and remote communication through the DIY library.

The DIY library changes consisted of adding routines that allow sending and receiving raw pointers directly and encapsulated in a newly introduced ``blob'' data type, which is roughly equivalent to the raw data buffers previously described for \vtkm's array management.
This contrasts the regular operation of DIY, which usually copies and serializes data before sending and receiving.
Additionally, we introduced a new API in DIY to control the ownership of the passed raw pointer.

The corresponding \vtkm changes consisted of modifying the \vtkm class that manages memory buffers and their location among host and devices.
In particular, the changes overwrite the DIY serialization to directly pass these memory buffers to and from DIY.
%Additionally, a new CMake option named \texttt{VTKm\_ENABLE\_GPU\_MPI} was added to activate direct GPU-to-GPU communication of \vtkm buffer objects.
This enables direct GPU communication in \vtkm because most of the \vtkm storage entities are composed of \vtkm buffers. 

The main challenge found during the implementation of this feature was maintaining compatibility with the Frontier target system.
Frontier's software stack was a moving target with frequent updates that---in many cases---required us to modify build and runtime parameters and---in some other cases---required us to change the \vtkm and DIY source code.
This challenge was minimized by provisioning the \vtkm GitLab project with nightly jobs to build and run tests on the Frontier test-bed system.
This extra testing allowed us to quickly identify problems that arose from changes to either \vtkm's source code or to the Frontier software stack.
